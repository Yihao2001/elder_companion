{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5d3151",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1ec09",
   "metadata": {},
   "source": [
    "# Load QA-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0252e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_weights/qa_stacked_hybrid_model.pkl\", \"rb\") as f:\n",
    "    stacked_model_qa = pickle.load(f)\n",
    "\n",
    "with open(\"model_weights/qa_tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "    tfidf_vectorizer_qa = pickle.load(f)\n",
    "\n",
    "with open(\"model_weights/qa_sbert_model_name.pkl\", \"rb\") as f:\n",
    "    sbert_model_name = pickle.load(f)\n",
    "    sbert_model_qa = SentenceTransformer(sbert_model_name)\n",
    "\n",
    "def prepare_features_qa(texts):\n",
    "\n",
    "    question_words = ['who','what','where','when','why','how','which']\n",
    "    def extract_simple_nlp_features(text):\n",
    "        words = text.lower().split()\n",
    "        return np.array([\n",
    "            int(text.lower().endswith('?')),                      \n",
    "            int(words[0] in question_words if words else 0)\n",
    "        ])\n",
    "        \n",
    "    X_tfidf_new = tfidf_vectorizer_qa.transform(texts).toarray()\n",
    "    X_sbert_new = sbert_model_qa.encode(texts, show_progress_bar=False)\n",
    "    X_nlp_new = np.array([extract_simple_nlp_features(t) for t in texts])\n",
    "    \n",
    "    return np.hstack([X_tfidf_new, X_sbert_new, X_nlp_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c068803",
   "metadata": {},
   "source": [
    "# Load Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d1f27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_weights/topic_log_reg_hybrid_model.pkl\", \"rb\") as f:\n",
    "    log_reg_model_topic = pickle.load(f)\n",
    "\n",
    "with open(\"model_weights/topic_tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "    tfidf_vectorizer_topic = pickle.load(f)\n",
    "\n",
    "with open(\"model_weights/topic_sbert_model_name.pkl\", \"rb\") as f:\n",
    "    sbert_model_name = pickle.load(f)\n",
    "    sbert_model_topic = SentenceTransformer(sbert_model_name)\n",
    "\n",
    "with open(\"model_weights/topic_label_encoder.pkl\", \"rb\") as f:\n",
    "    le_topic = pickle.load(f)\n",
    "\n",
    "with open(\"model_weights/topic_category_keywords.pkl\", \"rb\") as f:\n",
    "    CATEGORY_KEYWORDS = pickle.load(f)\n",
    "\n",
    "# --- Feature utilities ---\n",
    "def count_category_words_topic(text, category_words):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return sum(1 for w in words if w in category_words)\n",
    "\n",
    "def prepare_features_topic(texts):\n",
    "    X_tfidf = tfidf_vectorizer_topic.transform(texts).toarray()\n",
    "    X_sbert = sbert_model_topic.encode(texts, show_progress_bar=False)\n",
    "    category_features = np.array([\n",
    "        [\n",
    "            count_category_words_topic(t, CATEGORY_KEYWORDS['healthcare']),\n",
    "            count_category_words_topic(t, CATEGORY_KEYWORDS['longterm']),\n",
    "            count_category_words_topic(t, CATEGORY_KEYWORDS['shortterm'])\n",
    "        ]\n",
    "        for t in texts\n",
    "    ])\n",
    "    return np.hstack([X_tfidf, X_sbert, category_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20d1efc",
   "metadata": {},
   "source": [
    "# Make Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d21d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa682759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leege\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'When do I take my medicine?', 'llm_text': 'When do I take my medicine?', 'topic': 'healthcare', 'qa': 'question', 'llm_topic': 'healthcare', 'llm_qa': 'question', 'final_topic': 'healthcare', 'final_qa': 'question'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os, re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -----------------------------\n",
    "# Load API key\n",
    "# -----------------------------\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "gemini_client = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=1000,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# State definitions\n",
    "# -----------------------------\n",
    "class ClassicalState(TypedDict):\n",
    "    text: str\n",
    "    topic: str\n",
    "    qa: str\n",
    "\n",
    "class LLMState(TypedDict):\n",
    "    llm_text: str\n",
    "    llm_topic: str\n",
    "    llm_qa: str\n",
    "\n",
    "class FinalState(TypedDict):\n",
    "    text: str\n",
    "    llm_text: str\n",
    "    topic: str\n",
    "    qa: str\n",
    "    llm_topic: str\n",
    "    llm_qa: str\n",
    "    final_topic: str\n",
    "    final_qa: str\n",
    "\n",
    "# -----------------------------\n",
    "# Classical classifier nodes\n",
    "# -----------------------------\n",
    "def classify_text_topic(state: ClassicalState) -> ClassicalState:\n",
    "    X_features = prepare_features_topic([state[\"text\"]])\n",
    "    pred_int = log_reg_model_topic.predict(X_features)\n",
    "    state[\"topic\"] = le_topic.inverse_transform(pred_int)[0]\n",
    "    return state\n",
    "\n",
    "def classify_text_qa(state: ClassicalState) -> ClassicalState:\n",
    "    X_features = prepare_features_qa([state[\"text\"]])\n",
    "    pred_int = stacked_model_qa.predict(X_features)\n",
    "    state[\"qa\"] = \"question\" if pred_int[0] == 1 else \"statement\"\n",
    "    return state\n",
    "\n",
    "# -----------------------------\n",
    "# LLM node\n",
    "# -----------------------------\n",
    "def classify_with_llm(state: LLMState) -> LLMState:\n",
    "    prompt = (\n",
    "        f\"Classify the following text into topic \"\n",
    "        f\"(healthcare, long term, short term) and QA type (question/answer). \"\n",
    "        f\"Reply ONLY in the format: topic: ..., qa: ...\\n\\n{state['llm_text']}\"\n",
    "    )\n",
    "    llm_response = gemini_client.invoke([HumanMessage(content=prompt)])\n",
    "    content = llm_response.content\n",
    "\n",
    "    topic_match = re.search(r\"topic\\s*:\\s*(\\w+)\", content, re.IGNORECASE)\n",
    "    qa_match = re.search(r\"qa\\s*:\\s*(\\w+)\", content, re.IGNORECASE)\n",
    "\n",
    "    state[\"llm_topic\"] = topic_match.group(1) if topic_match else \"unknown\"\n",
    "    state[\"llm_qa\"] = qa_match.group(1) if qa_match else \"unknown\"\n",
    "    return state\n",
    "\n",
    "# -----------------------------\n",
    "# Judgement node: merge results\n",
    "# -----------------------------\n",
    "def judgement_node(classical: ClassicalState, llm: LLMState) -> FinalState:\n",
    "    return FinalState(\n",
    "        text=classical[\"text\"],\n",
    "        llm_text=llm[\"llm_text\"],\n",
    "        topic=classical[\"topic\"],\n",
    "        qa=classical[\"qa\"],\n",
    "        llm_topic=llm[\"llm_topic\"],\n",
    "        llm_qa=llm[\"llm_qa\"],\n",
    "        final_topic=llm[\"llm_topic\"] or classical[\"topic\"],\n",
    "        final_qa=llm[\"llm_qa\"] or classical[\"qa\"]\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Example execution\n",
    "# -----------------------------\n",
    "classical_state: ClassicalState = {\"text\": \"When do I take my medicine?\", \"topic\": \"\", \"qa\": \"\"}\n",
    "llm_state: LLMState = {\"llm_text\": \"When do I take my medicine?\", \"llm_topic\": \"\", \"llm_qa\": \"\"}\n",
    "\n",
    "# Run classical classifiers\n",
    "classical_state = classify_text_topic(classical_state)\n",
    "classical_state = classify_text_qa(classical_state)\n",
    "\n",
    "# Run LLM classifier\n",
    "llm_state = classify_with_llm(llm_state)\n",
    "\n",
    "# Merge results\n",
    "final_state = judgement_node(classical_state, llm_state)\n",
    "print(final_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
