{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b9da30",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01ea06",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\leege\\Documents\\Capstone\\elderly_conversational_sentences.csv\")\n",
    "df['binary_label'] = df['label'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Remove '?' to generalize model\n",
    "question_idx = df[df['text'].str.endswith('?')].index\n",
    "remove_idx = np.random.choice(question_idx, size=len(question_idx)//2, replace=False)\n",
    "df.loc[remove_idx, 'text'] = df.loc[remove_idx, 'text'].apply(lambda x: x.rstrip('?'))\n",
    "\n",
    "df['binary_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f79b9",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc91f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with tf-idf\n",
    "X_text = df['text'].values\n",
    "y = df['binary_label'].values\n",
    "\n",
    "# TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1,2))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X_text).toarray()\n",
    "\n",
    "# SBERT embeddings\n",
    "# all-MiniLM-L6-v2 or all-mpnet-base-v2\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "X_sbert = sbert_model.encode(X_text, show_progress_bar=True)\n",
    "\n",
    "# Simple NLP features\n",
    "question_words = ['who','what','where','when','why','how','which']\n",
    "def extract_simple_nlp_features(text):\n",
    "    words = text.lower().split()\n",
    "    return np.array([\n",
    "        int(text.lower().endswith('?')),                      \n",
    "        int(words[0] in question_words if words else 0)\n",
    "    ])\n",
    "X_nlp = np.array([extract_simple_nlp_features(t) for t in X_text])\n",
    "\n",
    "# Combine all features\n",
    "X_hybrid = np.hstack([X_tfidf, X_sbert, X_nlp])\n",
    "print(\"Hybrid feature shape:\", X_hybrid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5d631",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d062c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test Split\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_hybrid, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=42)\n",
    "\n",
    "# Calculate scale_pos_weight for imbalanced dataset\n",
    "neg, pos = np.bincount(y_train)\n",
    "scale_pos_weight = neg / pos\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=427,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.03317381190502595,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1792328642721363,\n",
    "    subsample=0.7483273008793065,\n",
    "    colsample_bytree=0.6296178606936361,\n",
    "    reg_alpha=0.2247253370691017,\n",
    "    reg_lambda=0.6908202329808226,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1746a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ff29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d773e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With tf-idf\n",
    "def predict_text(new_texts):\n",
    "    \"\"\"\n",
    "    Input: list of strings\n",
    "    Output: predicted binary labels (0=answer, 1=question)\n",
    "    \"\"\"\n",
    "\n",
    "    question_words = ['who','what','where','when','why','how','which']\n",
    "    def extract_simple_nlp_features(text):\n",
    "        words = text.lower().split()\n",
    "        return np.array([\n",
    "            int(text.lower().endswith('?')),                      \n",
    "            int(words[0] in question_words if words else 0)\n",
    "        ])\n",
    "        \n",
    "    X_tfidf_new = tfidf_vectorizer.transform(new_texts).toarray()\n",
    "    X_sbert_new = sbert_model.encode(new_texts, show_progress_bar=False)\n",
    "    X_nlp_new = np.array([extract_simple_nlp_features(t) for t in new_texts])\n",
    "    \n",
    "    X_new_hybrid = np.hstack([X_tfidf_new, X_sbert_new, X_nlp_new])\n",
    "    preds = xgb_model.predict(X_new_hybrid)\n",
    "    return preds\n",
    "\n",
    "predict_text([\"When is time now\"])\n",
    "# extract_simple_nlp_features(\"Where is medicine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extended test data with 20 samples\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Did you take your kopi from the kopitiam today\",\n",
    "        \"I remember when we used to walk to the hawker centre after school.\",\n",
    "        \"Can you help me top up my EZ-Link card\",\n",
    "        \"My knees have been hurting after walking at the park connector.\",\n",
    "        \"What time is the doctor’s appointment at NUH tomorrow\",\n",
    "        \"I enjoy watering the plants in my HDB balcony every morning.\",\n",
    "        \"Have you seen the news about the MRT delay this morning\",\n",
    "        \"I baked some pineapple tarts yesterday for CNY.\",\n",
    "        \"Do you know how to set the aircon timer properly\",\n",
    "        \"I like listening to old getai songs during the festive season.\",\n",
    "        \"Why did the bus arrive late at the bus stop near Bedok?\",\n",
    "        \"I feel tired after walking around the wet market.\",\n",
    "        \"Could you remind me of your birthday again so I can send angbao\",\n",
    "        \"I watched a really interesting documentary on Singapore’s history.\",\n",
    "        \"When are we visiting the grandchildren at Changi next\",\n",
    "        \"Did you manage to book a slot at the community centre for exercise\",\n",
    "        \"I miss the old hawker uncle at our neighbourhood market.\",\n",
    "        \"Can you teach me how to use WhatsApp video call\",\n",
    "        \"I went for a morning stroll along East Coast Park today.\",\n",
    "        \"Have you checked the weather before heading to the pasar malam tonight\"\n",
    "    ],\n",
    "    \"label\": [  # 1 = question, 0 = statement\n",
    "        1, 0, 1, 0, 1,\n",
    "        0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1,\n",
    "        1, 0, 1, 0, 1\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_test = pd.DataFrame(data)\n",
    "\n",
    "# Assuming predict_text function exists\n",
    "df_test['pred'] = df_test['text'].apply(lambda x: predict_text([x]))\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3594c6",
   "metadata": {},
   "source": [
    "# Save model (Change directory accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7592f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "with open(\"qa_xgb_hybrid_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "\n",
    "with open(\"qa_tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "with open(\"qa_sbert_model_name.pkl\", \"wb\") as f:\n",
    "    pickle.dump('all-MiniLM-L6-v2', f)\n",
    "\n",
    "print(\"Models and artifacts saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c61c28",
   "metadata": {},
   "source": [
    "# Randomized Search - Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1aa7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# 1. Parameter grid (you can expand ranges)\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 600),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'min_child_weight': randint(1, 6),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'reg_alpha': uniform(0, 0.5),\n",
    "    'reg_lambda': uniform(0.5, 2)\n",
    "}\n",
    "\n",
    "# 2. Base XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,  # number of random combinations to try\n",
    "    scoring='accuracy',\n",
    "    cv=3,       # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Fit to training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. Best parameters and score\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "print(\"Best CV accuracy:\", random_search.best_score_)\n",
    "\n",
    "# 6. Evaluate on test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
