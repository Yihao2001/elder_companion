{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e742a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError, validator\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Pydantic model for validation\n",
    "class ElderlySentence(BaseModel):\n",
    "    text: str\n",
    "    label: str\n",
    "\n",
    "# 2. Config\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "api_key = 'AIzaSyBlOeJT_o3tNN8XQfz0FOea9pK7gCJoQB8'\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# 3. Prompt template\n",
    "prompt_template = \"\"\"\n",
    "Generate a dataset of short conversational sentences suitable for elderly people in Singapore.\n",
    "- Each sentence should be natural, realistic, and reflect daily life in Singapore (e.g., HDB, kopitiam, MRT, wet market, CNY, angbao, getai, hawker centres, local shops, public transport, etc.).\n",
    "- Include a variety of situations: asking questions, making statements, sharing memories, commenting on local events, talking about health, family, food, or hobbies.\n",
    "- Each output must be exactly one sentence – do not add extra context, explanations, or follow-up sentences.\n",
    "- Categorise each sentence into one of three labels:\n",
    "  * \"healthcare\": health-related, medical visits, physical condition, medicine.\n",
    "  * \"long-term\": stable aspects of life like family members, hobbies, owning a car/house, or other things that don’t change quickly.\n",
    "  * \"short-term\": day-to-day or situational things like meals, weather, appointments, errands, public transport, or current events.\n",
    "- Output the dataset as a JSON array of objects with two keys: \"text\" and \"label\".\n",
    "- Generate up to 1000 examples per request.\n",
    "\n",
    "Example output:\n",
    "[\n",
    "    {\"text\": \"I need to collect my medicine from the polyclinic tomorrow.\", \"label\": \"healthcare\"},\n",
    "    {\"text\": \"My grandson just started secondary school this year.\", \"label\": \"long-term\"},\n",
    "    {\"text\": \"Shall we eat chicken rice at the hawker centre later?\", \"label\": \"short-term\"}\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 4. Function to parse JSON output and validate\n",
    "def clean_json_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove markdown code fences like ```json ... ``` from the generated text\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if text.startswith(\"```\") and text.endswith(\"```\"):\n",
    "        # Remove first and last lines if they are code fences\n",
    "        lines = text.splitlines()\n",
    "        if len(lines) >= 3 and lines[0].startswith(\"```\") and lines[-1].startswith(\"```\"):\n",
    "            return \"\\n\".join(lines[1:-1])\n",
    "    return text\n",
    "    \n",
    "def parse_and_validate_json(json_text: str) -> List[dict]:\n",
    "    json_text = clean_json_text(json_text)\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(json_text)  # <-- THIS was missing\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"JSON decoding error:\", e)\n",
    "        return []\n",
    "    \n",
    "    valid_rows = []\n",
    "    for item in data:\n",
    "        try:\n",
    "            validated = ElderlySentence(**item)\n",
    "            valid_rows.append(validated.model_dump())\n",
    "        except ValidationError:\n",
    "            print(\"Validation error, skipping item:\", item)\n",
    "    return valid_rows\n",
    "\n",
    "# 5. Generate multiple batches\n",
    "all_data = []\n",
    "num_batches = 5  # Adjust as needed\n",
    "\n",
    "for i in range(num_batches):\n",
    "    print(f\"Generating batch {i+1}/{num_batches}...\")\n",
    "    \n",
    "    # Create a GenerativeModel instance\n",
    "    model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "    \n",
    "    # Generate content\n",
    "    response = model.generate_content(\n",
    "        prompt_template,\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=1.2,   # default ~0.7, increase for more variety\n",
    "            top_p=0.9,         # nucleus sampling\n",
    "            top_k=40           # restricts to top-k tokens\n",
    "        )\n",
    "    )\n",
    "    # print(response.text)\n",
    "    \n",
    "    # Extract generated JSON\n",
    "    generated_text = response.text\n",
    "    batch_data = parse_and_validate_json(generated_text)\n",
    "    all_data.extend(batch_data)\n",
    "    \n",
    "    time.sleep(1)  # Polite delay\n",
    "\n",
    "# 6. Convert to pandas DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"Total rows generated:\", len(df))\n",
    "print(df.head())\n",
    "\n",
    "# 7. Save to CSV\n",
    "csv_path = \"elderly_topical_conversational_sentences.csv\"\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "\n",
    "    df_existing = pd.read_csv(csv_path)\n",
    "    df_combined = pd.concat([df_existing, df], ignore_index=True)\n",
    "\n",
    "    # Count duplicates before dropping\n",
    "    num_duplicates = df_combined.duplicated(subset=['text']).sum()\n",
    "    print(f\"Number of duplicate rows to be removed: {num_duplicates}\")\n",
    "    \n",
    "    df_combined.drop_duplicates(subset=['text'], inplace=True)\n",
    "    df_combined.to_csv(csv_path, index=False)\n",
    "    print(f\"Appended new data. Dataset now has {len(df_combined)} rows, saved to {csv_path}\")\n",
    "else:\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved dataset to {csv_path} with {len(df)} rows\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
