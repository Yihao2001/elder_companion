{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f0b0f1",
   "metadata": {},
   "source": [
    "# Insertion Agentic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2120946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "# --- State ---\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langchain_core.messages import AnyMessage\n",
    "import operator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- State ---\n",
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    # Use add_messages reducer for messages to handle concurrent appends\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "        \n",
    "    # These are single values, so they're fine as-is\n",
    "    has_context: bool\n",
    "    final_answer: str\n",
    "    # Add this to track the insertion agent message\n",
    "    insertion_agent_message: AnyMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac742669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=1000  # limiting the output affects the quality of tool calling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8f3b0",
   "metadata": {},
   "source": [
    "# Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68386b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import login\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import logging\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "CONNECTION_STRING = os.getenv(\"DATABASE_URL\")\n",
    "SECRET_KEY = os.getenv(\"DATABASE_ENCRYPTION_KEY\")\n",
    "\n",
    "engine = create_engine(\n",
    "    CONNECTION_STRING,\n",
    "    pool_size=5,\n",
    "    max_overflow=10,\n",
    "    pool_pre_ping=True,\n",
    "    pool_recycle=3600,\n",
    "    connect_args={\n",
    "        \"keepalives\": 1,\n",
    "        \"keepalives_idle\": 30,\n",
    "        \"keepalives_interval\": 10,\n",
    "        \"tcp_user_timeout\": 60000,\n",
    "    },\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "# Load model once globally\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bcaae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(text: str) -> list:\n",
    "    \"\"\"Generate embedding vector from text\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        raise ValueError(\"Query must be a non-empty string\")\n",
    "    return model.encode(text, normalize_embeddings=True).tolist()  # ✅ Normalize for cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37075202",
   "metadata": {},
   "source": [
    "# Setting Up Default Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5581bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating elderly profile...\n",
      "Profile created/verified with ID: 12345678-1234-1234-1234-012345678910\n"
     ]
    }
   ],
   "source": [
    "ELDERLY_ID = \"12345678-1234-1234-1234-012345678910\"\n",
    "\n",
    "# Elderly profile data\n",
    "profile_data = {\n",
    "    \"name\": \"Admiralty Bedok Canberra Tan\",\n",
    "    \"date_of_birth\": \"1965-01-01\",\n",
    "    \"gender\": \"Male\",  \n",
    "    \"nationality\": \"Singaporean\",\n",
    "    \"dialect_group\": \"Hokkien\",\n",
    "    \"marital_status\": \"Married\",\n",
    "    \"address\": \"38 Oxley Road, Singapore 238629\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(CONNECTION_STRING) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            print(\"Creating elderly profile...\")\n",
    "            \n",
    "            # Insert profile with fixed UUID\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO elderly_profile \n",
    "                (id, name, date_of_birth, gender, nationality, dialect_group, marital_status, address)\n",
    "                VALUES (%s, pgp_sym_encrypt(%s,%s), pgp_sym_encrypt(%s,%s), %s, \n",
    "                        pgp_sym_encrypt(%s,%s), pgp_sym_encrypt(%s,%s), %s, pgp_sym_encrypt(%s,%s))\n",
    "                ON CONFLICT (id) DO NOTHING;\n",
    "            \"\"\", (\n",
    "                ELDERLY_ID,\n",
    "                profile_data[\"name\"], SECRET_KEY,\n",
    "                profile_data[\"date_of_birth\"], SECRET_KEY,\n",
    "                profile_data[\"gender\"],\n",
    "                profile_data[\"nationality\"], SECRET_KEY,\n",
    "                profile_data[\"dialect_group\"], SECRET_KEY,\n",
    "                profile_data[\"marital_status\"],\n",
    "                profile_data[\"address\"], SECRET_KEY\n",
    "            ))\n",
    "            \n",
    "            print(f\"Profile created/verified with ID: {ELDERLY_ID}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd6718",
   "metadata": {},
   "source": [
    "# Insertion Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eba114",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc6f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_short_term(content: str, elderly_id: str = ELDERLY_ID) -> dict:\n",
    "    if not content or not content.strip():\n",
    "        return {\"success\": False, \"error\": \"content is required and cannot be empty.\"}\n",
    "\n",
    "    embedding = embed(content)\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text(\"\"\"\n",
    "                INSERT INTO short_term_memory (\n",
    "                    elderly_id, content, embedding\n",
    "                ) VALUES (\n",
    "                    :elderly_id, :content, :embedding\n",
    "                )\n",
    "                RETURNING id, created_at;\n",
    "            \"\"\")\n",
    "            result = conn.execute(query, {\n",
    "                \"elderly_id\": elderly_id.strip(),\n",
    "                \"content\": content.strip(),\n",
    "                \"embedding\": str(embedding)\n",
    "            }).fetchone()\n",
    "            conn.commit()\n",
    "\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"message\": \"Short-term memory stored successfully\",\n",
    "                \"record_id\": str(result.id),\n",
    "                \"created_at\": result.created_at.isoformat() if result.created_at else None,\n",
    "                \"embedding_provided\": embedding is not None\n",
    "            }\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"❌ Database error inserting STM: {str(e)}\")\n",
    "        return {\"success\": False, \"error\": f\"Database error: {str(e)}\"}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error inserting STM: {str(e)}\")\n",
    "        return {\"success\": False, \"error\": f\"Unexpected error: {str(e)}\"}\n",
    "\n",
    "\n",
    "\n",
    "def insert_long_term(category: str, key: str, value: str, elderly_id: str = ELDERLY_ID) -> dict:\n",
    "    if not category or not category.strip():\n",
    "        return {\"success\": False, \"error\": \"category is required and cannot be empty.\"}\n",
    "    if not key or not key.strip():\n",
    "        return {\"success\": False, \"error\": \"key is required and cannot be empty.\"}\n",
    "    if not value or not value.strip():\n",
    "        return {\"success\": False, \"error\": \"value is required and cannot be empty.\"}\n",
    "\n",
    "    embedding = embed(value)\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text(\"\"\"\n",
    "                INSERT INTO long_term_memory (\n",
    "                    elderly_id, category, key, value, embedding\n",
    "                ) VALUES (\n",
    "                    :elderly_id, :category, :key, :value, :embedding\n",
    "                )\n",
    "                RETURNING id, last_updated;\n",
    "            \"\"\")\n",
    "            result = conn.execute(query, {\n",
    "                \"elderly_id\": elderly_id.strip(),\n",
    "                \"category\": category.strip(),\n",
    "                \"key\": key.strip(),\n",
    "                \"value\": value.strip(),\n",
    "                \"embedding\": str(embedding)\n",
    "            }).fetchone()\n",
    "            conn.commit()\n",
    "\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"message\": \"Long-term memory stored successfully\",\n",
    "                \"record_id\": str(result.id),\n",
    "                \"last_updated\": result.last_updated.isoformat() if result.last_updated else None,\n",
    "                \"embedding_provided\": embedding is not None\n",
    "            }\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"❌ Database error inserting LTM: {str(e)}\")\n",
    "        return {\"success\": False, \"error\": f\"Database error: {str(e)}\"}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error inserting LTM: {str(e)}\")\n",
    "        return {\"success\": False, \"error\": f\"Unexpected error: {str(e)}\"}\n",
    "\n",
    "\n",
    "\n",
    "def insert_health_record(record_type: str, description: str, diagnosis_date: Optional[str] = None, elderly_id: str = ELDERLY_ID) -> dict:\n",
    "    if not record_type or not record_type.strip():\n",
    "        return {\"success\": False, \"error\": \"record_type is required and cannot be empty.\"}\n",
    "    if not description or not description.strip():\n",
    "        return {\"success\": False, \"error\": \"description is required and cannot be empty.\"}\n",
    "\n",
    "    # Validate date format if provided\n",
    "    if diagnosis_date:\n",
    "        try:\n",
    "            from datetime import datetime\n",
    "            datetime.strptime(diagnosis_date, \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            return {\"success\": False, \"error\": \"diagnosis_date must be in YYYY-MM-DD format if provided.\"}\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            query = text(\"\"\"\n",
    "                INSERT INTO healthcare_records (\n",
    "                    elderly_id, record_type, description, diagnosis_date, embedding\n",
    "                ) VALUES (\n",
    "                    :elderly_id, :record_type, :description, :diagnosis_date, :embedding\n",
    "                )\n",
    "                RETURNING id, last_updated;\n",
    "            \"\"\")\n",
    "            result = conn.execute(query, {\n",
    "                \"elderly_id\": elderly_id.strip(),\n",
    "                \"record_type\": record_type.strip(),\n",
    "                \"description\": description.strip(),\n",
    "                \"diagnosis_date\": diagnosis_date if diagnosis_date else None,\n",
    "                \"embedding\": str(embedding) if embedding else None\n",
    "            }).fetchone()\n",
    "            conn.commit()\n",
    "\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"message\": \"Healthcare record stored successfully\",\n",
    "                \"record_id\": str(result.id),\n",
    "                \"last_updated\": result.last_updated.isoformat() if result.last_updated else None,\n",
    "                \"embedding_provided\": embedding is not None,\n",
    "                \"diagnosis_date\": diagnosis_date\n",
    "            }\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"❌ Database error inserting health record: {str(e)}\")\n",
    "        return {\"success\": False, \"error\": f\"Database error: {str(e)}\"}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error inserting health record: {str(e)}\")\n",
    "        return {\"success\": False, \"error\": f\"Unexpected error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eaacc8",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cdf61d",
   "metadata": {},
   "source": [
    "#### pydantic for function calling schema for insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef98e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LTMCategories(str, Enum):\n",
    "    personal = \"personal\"\n",
    "    family = \"family\"\n",
    "    education = \"education\"\n",
    "    career = \"career\"\n",
    "    lifestyle = \"lifestyle\"\n",
    "    finance = \"finance\"\n",
    "    legal = \"legal\"\n",
    "\n",
    "\n",
    "class HealthRecordTypes(str, Enum):\n",
    "    condition = \"condition\"\n",
    "    procedure = \"procedure\"\n",
    "    appointment = \"appointment\"\n",
    "    medication = \"medication\"\n",
    "\n",
    "\n",
    "class InsertShortTermSchema(BaseModel):\n",
    "    content: str = Field(\n",
    "        ..., \n",
    "        description=\"Short-term conversational detail to store. Use for temporary information that's useful in the near future but doesn't belong in long-term or healthcare storage. Examples: reminders, temporary preferences, upcoming appointments, casual mentions.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class InsertLongTermSchema(BaseModel):\n",
    "    category: LTMCategories = Field(\n",
    "        ..., \n",
    "        description=\"Category of long-term memory. Use for stable traits & preferences that rarely change - generally fixed profile information. Must follow one of the following categories ['personal','family','education','career','lifestyle','finance','legal']\"\n",
    "    )\n",
    "    key: str = Field(\n",
    "        ..., \n",
    "        description=\"Key or label for the memory fact. Should be a clear, descriptive subcategory for the category for this piece of long-term information\"\n",
    "    )\n",
    "    value: str = Field(\n",
    "        ..., \n",
    "        description=\"The fact/value to store. The actual free form user information long-term memory item. This should be stable information that rarely changes.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class InsertHealthSchema(BaseModel):\n",
    "    record_type: HealthRecordTypes = Field(\n",
    "        ..., \n",
    "        description=\"Type of healthcare record. Use for official physical/mental health information explicitly shared for future care. Must follow one of the following categories ['condition','procedure','appointment','medication']\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ..., \n",
    "        description=\"The details of the healthcare record. The actual free form and complete description of the medical information being stored. Should include specific details relevant to the record type.\"\n",
    "    )\n",
    "    diagnosis_date: Optional[str] = Field(\n",
    "        None, \n",
    "        description=\"Date in YYYY-MM-DD format (optional). When the healthcare event occurred, was diagnosed, or is scheduled. Leave empty if no specific date was mentioned. Examples: '2023-12-15', '2024-03-20', null.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e4d65",
   "metadata": {},
   "source": [
    "#### @tool to wrap over core insertion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb96c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(args_schema=InsertShortTermSchema)\n",
    "def insert_short_term_tool(content: str) -> str:\n",
    "    \"\"\"Insert a short-term memory item.\"\"\"\n",
    "    result = insert_short_term(content=content)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "@tool(args_schema=InsertLongTermSchema)\n",
    "def insert_long_term_tool(category: LTMCategories, key: str, value: str) -> str:\n",
    "    \"\"\"Insert a long-term memory fact (stable traits, demographics, preferences).\"\"\"\n",
    "    result = insert_long_term(category=category, key=key, value=value)\n",
    "    return str(result)\n",
    "\n",
    "\n",
    "@tool(args_schema=InsertHealthSchema)\n",
    "def insert_health_tool(record_type: HealthRecordTypes, description: str, diagnosis_date: Optional[str] = None) -> str:\n",
    "    \"\"\"Insert a healthcare record (conditions, medications, appointments).\"\"\"\n",
    "    result = insert_health_record(record_type=record_type, description=description, diagnosis_date=diagnosis_date)\n",
    "    return str(result)\n",
    "\n",
    "insertion_tools = [insert_long_term_tool, insert_health_tool, insert_short_term_tool]\n",
    "insertion_llm = llm.bind_tools(insertion_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c02b7",
   "metadata": {},
   "source": [
    "# Insertion Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38693033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- 1.1  System prompt for the storage agent -------------\n",
    "INSERTION_SYSTEM = \"\"\"\n",
    "    ## Role  \n",
    "    You are memory agent involved in deciding what information is important to store and in the right place. Only store what is neccessary.\n",
    "    If you decide to store information, Long-Term and Healthcare are for official and formal information, general miscellaneous and all other information should be stored in short term\n",
    "    \n",
    "    --------------------------------------------------\n",
    "    OBJECTIVES  \n",
    "    1. Extract ONLY relevant user information from the conversation.  \n",
    "    2. Decide if any item is worth storing.  \n",
    "    3. If YES → store all relevant information by calling the matching tool(s).  \n",
    "    4. If NO → do nothing (silent pass).\n",
    "\n",
    "    --------------------------------------------------\n",
    "    BUCKETS \n",
    "\n",
    "    1. LONG-TERM (ltm)  \n",
    "    Content: stable traits & preferences that rarely change, generally fixed profile information\n",
    "    Examples:  \n",
    "    - name, preferred_name, date_of_birth, gender, address  \n",
    "    - food/activity/music/hobby preferences (category + value)  \n",
    "    - family & social relationships (contact_name, relationship_type, is_emergency_contact)  \n",
    "    - life memories (memory_title, memory_content, memory_category)  \n",
    "    - daily routines (routine_name, time_of_day, frequency)\n",
    "\n",
    "    2. HEALTH-CARE (hcm)  \n",
    "    Tables: Official medical_records, medications, medical_conditions, allergies, diagnosis  \n",
    "    Content: physical/mental health info explicitly shared for future care\n",
    "    Examples:  \n",
    "    - diagnoses, lab results, vital signs (medical_records)  \n",
    "    - medication_name, dosage, frequency (medications)  \n",
    "    - condition_name, severity, status (medical_conditions)  \n",
    "    - allergen, reaction_type (allergies)\n",
    "    - official tied medical facility (polyclinics, hospitals, family clinics, general practioners)\n",
    "\n",
    "    3. GENERAL / SHORT-TERM  \n",
    "    Tables: memory_contexts (context_summary)  \n",
    "    Content: conversational details useful in the near future\n",
    "    Examples:  \n",
    "    - “I have a cardiology visit next Tuesday”  \n",
    "    - “Please remind me to call my grandson tonight”  \n",
    "    - “I prefer chicken for dinner today”\n",
    "\n",
    "    --------------------------------------------------\n",
    "    RULES  \n",
    "    - You may call multiple tools in one go if needed.\n",
    "    - Only store what’s explicitly shared and matches a bucket.\n",
    "\"\"\"\n",
    "\n",
    "# ------------- 1.2  ReAct agent (no custom state_schema) -------------\n",
    "react_insertion_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=insertion_tools\n",
    ")\n",
    "\n",
    "def react_insertion_node(state: AgentState):\n",
    "    system = SystemMessage(content=INSERTION_SYSTEM)\n",
    "    input_msg = HumanMessage(content=state[\"user_input\"])\n",
    "    react_result = react_insertion_agent.invoke({\"messages\": [system, input_msg]})\n",
    "\n",
    "    last_ai = next(m for m in reversed(react_result[\"messages\"]) if isinstance(m, AIMessage))\n",
    "\n",
    "    return {\n",
    "        \"messages\": react_result[\"messages\"],\n",
    "        \"insertion_actions\": [],\n",
    "        \"insertion_agent_message\": last_ai\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5c7f3",
   "metadata": {},
   "source": [
    "# Agentic Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "641db3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional path\n",
    "def route_insertion(state: AgentState):\n",
    "    # Get the last message (should be from Insertion Agent)\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if messages:\n",
    "        last_message = messages[-1]  # Get the Insertion Agent's response\n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            print(f\"[TOOL CALL] Routing to execute_insertion, tool_calls: {len(last_message.tool_calls)}\")\n",
    "            return \"execute_insertion\"\n",
    "    \n",
    "    print(\"[END] Routing to end\")\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49d3e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tool Nodes ---\n",
    "insertion_tool_node = ToolNode(insertion_tools)\n",
    "\n",
    "# --- Graph ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"Insertion_Agent\", react_insertion_node)\n",
    "workflow.add_node(\"execute_insertion\", insertion_tool_node)\n",
    "\n",
    "# --- Edges ---\n",
    "workflow.add_edge(START, \"Insertion_Agent\")\n",
    "\n",
    "# Conditional routing: either execute insertion or go straight to END\n",
    "workflow.add_conditional_edges(\n",
    "    \"Insertion_Agent\",\n",
    "    route_insertion,\n",
    "    {\n",
    "        \"execute_insertion\": \"execute_insertion\",\n",
    "        \"end\": END,\n",
    "    }\n",
    ")\n",
    "\n",
    "# After insertion, go to END\n",
    "workflow.add_edge(\"execute_insertion\", END)\n",
    "\n",
    "# --- Compile ---\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b504d87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m display(Image(\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - National University of Singapore\\Current Sem\\Capstone\\ElderCompanion\\elder_companion_rag_module\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:695\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    689\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    690\u001b[39m     curve_style=curve_style,\n\u001b[32m    691\u001b[39m     node_colors=node_colors,\n\u001b[32m    692\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    693\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    694\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - National University of Singapore\\Current Sem\\Capstone\\ElderCompanion\\elder_companion_rag_module\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:294\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    288\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    289\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    290\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    291\u001b[39m         )\n\u001b[32m    292\u001b[39m     )\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    302\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - National University of Singapore\\Current Sem\\Capstone\\ElderCompanion\\elder_companion_rag_module\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:451\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    447\u001b[39m     msg = (\n\u001b[32m    448\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    449\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    455\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed062307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_state(user_input: str) -> AgentState:\n",
    "    return {\n",
    "        \"user_input\": user_input,\n",
    "        \"messages\": [],  # 👈 explicitly start empty\n",
    "        \"final_answer\": \"\",\n",
    "        \"insertion_actions\": [],  # Initialize as empty list\n",
    "        \"insertion_agent_message\": None,\n",
    "        \"has_context\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fefeb",
   "metadata": {},
   "source": [
    "# Testing the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96cd671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[END] Routing to end\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I stay in 38 Oxley Road, Singapore 238629\"\n",
    "result = graph.invoke(create_initial_state(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16e23206",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = result.get(\"messages\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f443c167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"\",\n",
      "  \"additional_kwargs\": {\n",
      "    \"function_call\": {\n",
      "      \"name\": \"insert_long_term_tool\",\n",
      "      \"arguments\": \"{\\\"category\\\": \\\"personal\\\", \\\"value\\\": \\\"38 Oxley Road, Singapore 238629\\\", \\\"key\\\": \\\"address\\\"}\"\n",
      "    }\n",
      "  },\n",
      "  \"response_metadata\": {\n",
      "    \"prompt_feedback\": {\n",
      "      \"block_reason\": 0,\n",
      "      \"safety_ratings\": []\n",
      "    },\n",
      "    \"finish_reason\": \"STOP\",\n",
      "    \"model_name\": \"gemini-2.5-flash\",\n",
      "    \"safety_ratings\": []\n",
      "  },\n",
      "  \"type\": \"ai\",\n",
      "  \"name\": null,\n",
      "  \"id\": \"run--47ea0212-1da1-411b-9045-90149ef20dfc-0\",\n",
      "  \"example\": false,\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"name\": \"insert_long_term_tool\",\n",
      "      \"args\": {\n",
      "        \"category\": \"personal\",\n",
      "        \"value\": \"38 Oxley Road, Singapore 238629\",\n",
      "        \"key\": \"address\"\n",
      "      },\n",
      "      \"id\": \"108ffb63-d733-41c7-8aca-6278607551d1\",\n",
      "      \"type\": \"tool_call\"\n",
      "    }\n",
      "  ],\n",
      "  \"invalid_tool_calls\": [],\n",
      "  \"usage_metadata\": {\n",
      "    \"input_tokens\": 964,\n",
      "    \"output_tokens\": 88,\n",
      "    \"total_tokens\": 1052,\n",
      "    \"input_token_details\": {\n",
      "      \"cache_read\": 0\n",
      "    },\n",
      "    \"output_token_details\": {\n",
      "      \"reasoning\": 46\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leeee\\AppData\\Local\\Temp\\ipykernel_39840\\773403086.py:3: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print(json.dumps(x.dict(), indent=2))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(x.dict(), indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
