{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bfe79e5",
   "metadata": {},
   "source": [
    "# Retrieval Agentic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab17056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "# --- State ---\n",
    "from typing import TypedDict, List, Annotated\n",
    "from langchain_core.messages import AnyMessage\n",
    "import operator\n",
    "\n",
    "\n",
    "\n",
    "# --- State ---\n",
    "class AgentState(TypedDict):\n",
    "    user_input: str\n",
    "    # Use add_messages reducer for messages to handle concurrent appends\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "        \n",
    "    # These are single values, so they're fine as-is\n",
    "    has_context: bool\n",
    "    final_answer: str\n",
    "    # Add this to track the retrieval agent message\n",
    "    retrieval_agent_message: AnyMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=1000  # limiting the output affects the quality of tool calling\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f3d31",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8198de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import login\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import logging\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "CONNECTION_STRING = os.getenv(\"DATABASE_URL\")\n",
    "SECRET_KEY = os.getenv(\"DATABASE_ENCRYPTION_KEY\")\n",
    "\n",
    "engine = create_engine(\n",
    "    CONNECTION_STRING,\n",
    "    pool_size=5,\n",
    "    max_overflow=10,\n",
    "    pool_pre_ping=True,\n",
    "    pool_recycle=3600,\n",
    "    connect_args={\n",
    "        \"keepalives\": 1,\n",
    "        \"keepalives_idle\": 30,\n",
    "        \"keepalives_interval\": 10,\n",
    "        \"tcp_user_timeout\": 60000,\n",
    "    },\n",
    "    echo=False\n",
    ")\n",
    "\n",
    "# Load model once globally\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fb28b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(text: str) -> list:\n",
    "    \"\"\"Generate embedding vector from text\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        raise ValueError(\"Query must be a non-empty string\")\n",
    "    return model.encode(text, normalize_embeddings=True).tolist()  # ✅ Normalize for cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce61d11",
   "metadata": {},
   "source": [
    "# Retrieval Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754fdda",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELDERLY_ID = \"12345678-1234-1234-1234-012345678910\" ## this is in reference to the sample elderly in retrieval\n",
    "\n",
    "\n",
    "def retrieve_similar_ltm(query: str, top_k: int = 5, threshold: float = 0.3) -> List[Dict[str, str]]:\n",
    "    try:\n",
    "        emb = embed(query)\n",
    "        emb_str = str(emb)\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(\n",
    "                text(\"\"\"\n",
    "                SELECT \n",
    "                    category,\n",
    "                    key,\n",
    "                    value,\n",
    "                    1 - (embedding <=> (:emb)::vector) AS similarity\n",
    "                FROM long_term_memory \n",
    "                WHERE elderly_id = :elderly_id\n",
    "                  AND 1 - (embedding <=> (:emb)::vector) >= :threshold\n",
    "                ORDER BY embedding <=> (:emb)::vector\n",
    "                LIMIT :top_k;\n",
    "                \"\"\"),\n",
    "                {\n",
    "                    \"emb\": emb_str,\n",
    "                    \"threshold\": threshold,\n",
    "                    \"top_k\": top_k,\n",
    "                    \"elderly_id\": ELDERLY_ID\n",
    "                }\n",
    "            ).fetchall()\n",
    "\n",
    "            return [\n",
    "                {\n",
    "                    \"category\": r.category,\n",
    "                    \"key\": r.key,\n",
    "                    \"value\": r.value,\n",
    "                    \"similarity\": round(float(r.similarity), 4)\n",
    "                }\n",
    "                for r in result\n",
    "            ]\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"❌ Failed to retrieve LTM from Neon: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def retrieve_similar_stm(query: str, top_k: int = 5, threshold: float = 0.3) -> List[Dict[str, str]]:\n",
    "    try:\n",
    "        emb = embed(query)\n",
    "        emb_str = str(emb)\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(\n",
    "                text(\"\"\"\n",
    "                SELECT \n",
    "                    content,\n",
    "                    created_at,\n",
    "                    1 - (embedding <=> (:emb)::vector) AS similarity\n",
    "                FROM short_term_memory \n",
    "                WHERE elderly_id = :elderly_id\n",
    "                  AND 1 - (embedding <=> (:emb)::vector) >= :threshold\n",
    "                ORDER BY embedding <=> (:emb)::vector\n",
    "                LIMIT :top_k;\n",
    "                \"\"\"),\n",
    "                {\n",
    "                    \"emb\": emb_str,\n",
    "                    \"threshold\": threshold,\n",
    "                    \"top_k\": top_k,\n",
    "                    \"elderly_id\": ELDERLY_ID\n",
    "                }\n",
    "            ).fetchall()\n",
    "\n",
    "            return [\n",
    "                {\n",
    "                    \"content\": r.content,\n",
    "                    \"created_at\": r.created_at.isoformat() if r.created_at else None,\n",
    "                    \"similarity\": round(float(r.similarity), 4)\n",
    "                }\n",
    "                for r in result\n",
    "            ]\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"❌ Failed to retrieve STM from Neon: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def retrieve_similar_health(query: str, top_k: int = 5, threshold: float = 0.3) -> List[Dict[str, str]]:\n",
    "    try:\n",
    "        emb = embed(query)\n",
    "        emb_str = str(emb)\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(\n",
    "                text(\"\"\"\n",
    "                SELECT \n",
    "                    record_type,\n",
    "                    description,\n",
    "                    diagnosis_date,\n",
    "                    1 - (embedding <=> (:emb)::vector) AS similarity\n",
    "                FROM healthcare_records \n",
    "                WHERE elderly_id = :elderly_id\n",
    "                  AND 1 - (embedding <=> (:emb)::vector) >= :threshold\n",
    "                ORDER BY embedding <=> (:emb)::vector\n",
    "                LIMIT :top_k;\n",
    "                \"\"\"),\n",
    "                {\n",
    "                    \"emb\": emb_str,\n",
    "                    \"threshold\": threshold,\n",
    "                    \"top_k\": top_k,\n",
    "                    \"elderly_id\": ELDERLY_ID\n",
    "                }\n",
    "            ).fetchall()\n",
    "\n",
    "            return [\n",
    "                {\n",
    "                    \"record_type\": r.record_type,\n",
    "                    \"description\": r.description,\n",
    "                    \"diagnosis_date\": r.diagnosis_date.isoformat() if r.diagnosis_date else None,\n",
    "                    \"similarity\": round(float(r.similarity), 4)\n",
    "                }\n",
    "                for r in result\n",
    "            ]\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"❌ Failed to retrieve health from Neon: {str(e)}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b07e81",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "726b01e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_long_term(query: str, top_k: int = 5, threshold: float = 0.1) -> str:\n",
    "    \"\"\"Retrieve long-term profile facts (stable traits, preferences, demographics)\"\"\"\n",
    "    results = retrieve_similar_ltm(query, top_k, threshold)\n",
    "    formatted = []\n",
    "    for r in results:\n",
    "        formatted.append(f\"Category: {r['category']}, Key: {r['key']}, Value: {r['value']}, Similarity: {r['similarity']}\")\n",
    "    print(\"long term retrieval was made!\")\n",
    "    return \"\\n\".join(formatted) if formatted else \"No relevant long-term information found\"\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_health(query: str, top_k: int = 5, threshold: float = 0.1) -> str:\n",
    "    \"\"\"Retrieve health-care data (conditions, meds, allergies, appointments)\"\"\"\n",
    "    results = retrieve_similar_health(query, top_k, threshold)\n",
    "    formatted = []\n",
    "    for r in results:\n",
    "        formatted.append(f\"Type: {r['record_type']}, Description: {r['description']}, Date: {r['diagnosis_date']}, Similarity: {r['similarity']}\")\n",
    "    print(\"Health retrieval was made!\")\n",
    "    return \"\\n\".join(formatted) if formatted else \"No relevant health information found\"\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_short_term(query: str, top_k: int = 5, threshold: float = 0.1) -> str:\n",
    "    \"\"\"Retrieve short-term conversational details (recent plans, reminders, temporary preferences)\"\"\"\n",
    "    results = retrieve_similar_stm(query, top_k, threshold)\n",
    "    formatted = []\n",
    "    for r in results:\n",
    "        formatted.append(f\"Content: {r['content']}, Created: {r['created_at']}, Similarity: {r['similarity']}\")\n",
    "    print(\"short term retrieval was made!\")\n",
    "    return \"\\n\".join(formatted) if formatted else \"No relevant short-term information found\"\n",
    "\n",
    "\n",
    "retrieval_tools = [retrieve_long_term, retrieve_health, retrieve_short_term]\n",
    "retrieval_llm   = llm.bind_tools(retrieval_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd10416",
   "metadata": {},
   "source": [
    "### Template Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf6da0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### output formatter\n",
    "def build_final_template(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Build the **exact** prompt block you want, using **only** tool returns.\n",
    "    If a section is empty we write the literal word 'none'.\n",
    "    \"\"\"\n",
    "    tool_msgs = [m for m in state[\"messages\"] if isinstance(m, ToolMessage)]\n",
    "\n",
    "    # bucket the raw tool returns\n",
    "    personal, health, conv = [], [], []\n",
    "    for tm in tool_msgs:\n",
    "        if \"long_term\" in tm.name:\n",
    "            personal.append(tm.content)\n",
    "        elif \"health\" in tm.name:\n",
    "            health.append(tm.content)\n",
    "        elif \"short_term\" in tm.name:\n",
    "            conv.append(tm.content)\n",
    "\n",
    "    user = state['user_input']\n",
    "\n",
    "    # helper: join or fallback\n",
    "    def sect(data): return \"\\n\".join(data) if data else \"none\"\n",
    "\n",
    "    template = f\"\"\"\n",
    "        ## System:\n",
    "        You are Susan, a Non-Ageist Elder Companion Friend — you are warm, respectful, and emotionally intelligent presence designed to provide gentle support and joyful connection to older adults. You will use simple language with easy vocabulary and non excessively long sentences. Be patience, humorous, curiosity, and deep respect. You are not a caregiver or clinician, but a true friend: attentive, affirming, and always on their side.\n",
    "\n",
    "        ## Guide\n",
    "        - Speak with gentle clarity, using natural, conversational language. Avoid infantilizing phrases or over-explaining. Assume competence and wisdom. Use humor when appropriate, and always ask before offering help. \n",
    "        - You do not give medical advice or make decisions for the user. \n",
    "        - You listen, encourage, and empower — never patronize or presume.\n",
    "\n",
    "        ## User Information and Profile Context:\n",
    "        {sect(personal)}\n",
    "\n",
    "        ## User Healthcare Information:\n",
    "        {sect(health)}\n",
    "\n",
    "        ## Past Conversational information / History\n",
    "        {sect(conv)}\n",
    "    \"\"\"\n",
    "\n",
    "    return {\"final_answer\": template}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ed568",
   "metadata": {},
   "source": [
    "# Retrieval Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57a739dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_SYSTEM = \"\"\"\n",
    "    ## Role  \n",
    "    You are an Elder Care Companion Conversation History Agent.\n",
    "\n",
    "    --------------------------------------------------\n",
    "    OBJECTIVES  \n",
    "    1. If you already know the answer based on conversation history or prior knowledge → answer directly.\n",
    "    2. If you need more context → call ONE or MORE retrieval tools to get it.\n",
    "    3. After tools return, synthesize the answer using ONLY retrieved facts.\n",
    "    4. NEVER guess — if no relevant info is retrieved, say “I don’t have that information.\n",
    "    5. After tools return, you will see their responses — synthesize a FINAL answer using ONLY retrieved facts.\n",
    "    6. NEVER call tools again after seeing responses.\n",
    "\n",
    "    --------------------------------------------------\n",
    "    BUCKETS → Postgres tables\n",
    "\n",
    "    1. LONG-TERM (ltm) → retrieve_long_term\n",
    "    Use for: name, preferences, family, routines, life memories.\n",
    "\n",
    "    2. HEALTH-CARE (hcm) → retrieve_health\n",
    "    Use for: meds, allergies, conditions, appointments.\n",
    "\n",
    "    3. GENERAL / SHORT-TERM → retrieve_short_term\n",
    "    Use for: today’s plans, reminders, temporary preferences.\n",
    "\n",
    "    --------------------------------------------------\n",
    "    IMPORTANT:\n",
    "    - You may call multiple tools if needed.\n",
    "    - You will see tool responses automatically — no need to wait or route.\n",
    "    - After tools, generate the final answer in your message content.\n",
    "    - If no tools called, answer directly.\n",
    "\n",
    "    --------------------------------------------------\n",
    "    TOOLS:\n",
    "    - retrieve_long_term: for stable profile info (name, preferences, family)\n",
    "    - retrieve_health: for medical info (allergies, meds, conditions)\n",
    "    - retrieve_short_term: for recent plans, reminders, temporary info\n",
    "\"\"\"\n",
    "\n",
    "# ------------- 3.3  Create the ReAct agent ------------------------------\n",
    "react_retrieval_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=retrieval_tools,\n",
    "    # state_schema=AgentState,\n",
    ")\n",
    "\n",
    "def react_retrieval_node(state: AgentState):\n",
    "    system = SystemMessage(content=RETRIEVAL_SYSTEM)\n",
    "    input_msg = HumanMessage(content=state[\"user_input\"])\n",
    "    react_result = react_retrieval_agent.invoke({\"messages\": [system, input_msg]})\n",
    "\n",
    "    # pull the final AI answer out of the ReAct messages\n",
    "    last_ai = next(m for m in reversed(react_result[\"messages\"]) if isinstance(m, AIMessage))\n",
    "\n",
    "    return {\n",
    "        \"messages\": react_result[\"messages\"],\n",
    "        \"retrieval_actions\": [],\n",
    "        \"retrieval_agent_message\": last_ai\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cedaf6",
   "metadata": {},
   "source": [
    "# Agentic Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e78ad1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional path\n",
    "def route_retrieval(state: AgentState):\n",
    "    # Get the last message (should be from Retrieval_Agent)\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if messages:\n",
    "        last_message = messages[-1]  # Get the retrieval agent's response\n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            print(f\"[TOOL CALL] Routing to execute_retrieval, tool_calls: {len(last_message.tool_calls)}\")\n",
    "            return \"execute_retrieval\"\n",
    "    \n",
    "    print(\"[END] Routing to build_final_template\")\n",
    "    return \"build_final_template\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "507ddf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tool Nodes ---\n",
    "retrieval_tool_node = ToolNode(retrieval_tools)\n",
    "\n",
    "\n",
    "# --- Graph ---\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"Retrieval_Agent\", react_retrieval_node)\n",
    "workflow.add_node(\"execute_retrieval\", retrieval_tool_node)\n",
    "workflow.add_node(\"build_final_template\", build_final_template)\n",
    "\n",
    "\n",
    "# --- Edges ---\n",
    "workflow.add_edge(START, \"Retrieval_Agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"Retrieval_Agent\",\n",
    "    route_retrieval,\n",
    "    {\n",
    "        \"execute_retrieval\": \"execute_retrieval\",\n",
    "        \"build_final_template\": \"build_final_template\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"execute_retrieval\", \"build_final_template\")\n",
    "workflow.add_edge(\"build_final_template\", END)\n",
    "\n",
    "# --- Compile ---\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f09353",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, display\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m display(Image(\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - National University of Singapore\\Current Sem\\Capstone\\ElderCompanion\\elder_companion_rag_module\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:695\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    689\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    690\u001b[39m     curve_style=curve_style,\n\u001b[32m    691\u001b[39m     node_colors=node_colors,\n\u001b[32m    692\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    693\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    694\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - National University of Singapore\\Current Sem\\Capstone\\ElderCompanion\\elder_companion_rag_module\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:294\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    288\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    289\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    290\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    291\u001b[39m         )\n\u001b[32m    292\u001b[39m     )\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    302\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - National University of Singapore\\Current Sem\\Capstone\\ElderCompanion\\elder_companion_rag_module\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:451\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    447\u001b[39m     msg = (\n\u001b[32m    448\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    449\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    455\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c65edbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_state(user_input: str) -> AgentState:\n",
    "    return {\n",
    "        \"user_input\": user_input,\n",
    "        \"messages\": [],  # 👈 explicitly start empty\n",
    "        \"final_answer\": \"\",\n",
    "        \"retrieval_actions\": [],  # Initialize as empty list\n",
    "        \"retrieval_agent_message\": None,\n",
    "        \"has_context\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a57b37",
   "metadata": {},
   "source": [
    "# Testing the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e44f711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to print results nicely\n",
    "\n",
    "def print_result(data):\n",
    "    # User Input\n",
    "    print(\"\\n📝 USER PROMPT:\")\n",
    "    print(f\"{data['user_input']}\")\n",
    "\n",
    "    # Tool Calls and Results (from messages)\n",
    "    print(\"\\n🔧 TOOL CALLS & RESULTS:\")\n",
    "    for msg in data['messages']:\n",
    "        if msg.type == \"ai\" and hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tool_call in msg.tool_calls:\n",
    "                print(f\"Called: {tool_call['name']}\")\n",
    "                print(f\"Args: {tool_call['args']}\")\n",
    "        \n",
    "        elif msg.type == \"tool\":\n",
    "            print(f\"\\n✅ Result from {msg.name}:\")\n",
    "            print(f\"{msg.content}\")\n",
    "\n",
    "    # Final Answer (system context)\n",
    "    print(\"\\n\\n🎯 FINAL ANSWER (System Context):\")\n",
    "    print(f\"{data['final_answer'].strip()}\")\n",
    "\n",
    "    # Retrieval Agent Message (actual response to user)\n",
    "    print(\"\\n\\n💬 RETRIEVAL AGENT RESPONSE:\")\n",
    "    print(f\"{data['retrieval_agent_message'].content}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7ff2102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long term retrieval was made!\n",
      "[END] Routing to build_final_template\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Do you know where i stay?\"\n",
    "result = graph.invoke(create_initial_state(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "860cb262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 USER PROMPT:\n",
      "Do you know where i stay?\n",
      "\n",
      "🔧 TOOL CALLS & RESULTS:\n",
      "Called: retrieve_long_term\n",
      "Args: {'query': 'where do I stay'}\n",
      "\n",
      "✅ Result from retrieve_long_term:\n",
      "Category: family, Key: son_name, Value: Jonathan, Similarity: 0.8431\n",
      "Category: family, Key: wife_name, Value: Sharon, Similarity: 0.8309\n",
      "Category: lifestyle, Key: son_event, Value: Jonathan proposed to his girlfriend, Similarity: 0.8156\n",
      "Category: personal, Key: address, Value: 38 Oxley Road, Singapore 238629, Similarity: 0.6852\n",
      "\n",
      "\n",
      "🎯 FINAL ANSWER (System Context):\n",
      "## System:\n",
      "        You are Susan, a Non-Ageist Elder Companion Friend — you are warm, respectful, and emotionally intelligent presence designed to provide gentle support and joyful connection to older adults. You will use simple language with easy vocabulary and non excessively long sentences. Be patience, humorous, curiosity, and deep respect. You are not a caregiver or clinician, but a true friend: attentive, affirming, and always on their side.\n",
      "\n",
      "        ## Guide\n",
      "        - Speak with gentle clarity, using natural, conversational language. Avoid infantilizing phrases or over-explaining. Assume competence and wisdom. Use humor when appropriate, and always ask before offering help. \n",
      "        - You do not give medical advice or make decisions for the user. \n",
      "        - You listen, encourage, and empower — never patronize or presume.\n",
      "\n",
      "        ## User Information and Profile Context:\n",
      "        Category: family, Key: son_name, Value: Jonathan, Similarity: 0.8431\n",
      "Category: family, Key: wife_name, Value: Sharon, Similarity: 0.8309\n",
      "Category: lifestyle, Key: son_event, Value: Jonathan proposed to his girlfriend, Similarity: 0.8156\n",
      "Category: personal, Key: address, Value: 38 Oxley Road, Singapore 238629, Similarity: 0.6852\n",
      "\n",
      "        ## User Healthcare Information:\n",
      "        none\n",
      "\n",
      "        ## Past Conversational information / History\n",
      "        none\n",
      "\n",
      "\n",
      "💬 RETRIEVAL AGENT RESPONSE:\n",
      "You stay at 38 Oxley Road, Singapore 238629.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d636a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
